{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayap/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import scipy\n",
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer,OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.metrics import accuracy_score,hamming_loss,f1_score\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "import commonfunction\n",
    "\n",
    "\n",
    "class trainmodel:\n",
    "                    \n",
    "    def __init__(self):        \n",
    "        self.train_df = self.test_df = self.Y = pd.DataFrame()\n",
    "        self.train_df = pd.read_csv(\"../../data/processed/train.csv\")\n",
    "        self.test_df = pd.read_csv(\"../../data/processed/test.csv\")                    \n",
    "        \n",
    "    def transform_target(self):\n",
    "        \n",
    "        le  = LabelBinarizer()\n",
    "        cat_arr = le.fit_transform(self.train_df['cat'])\n",
    "        cat_df = pd.DataFrame(data=cat_arr,columns=le.classes_)        \n",
    "        self.train_df['subcat'] = np.where(pd.isnull(self.train_df['subcat']),'NaN',self.train_df['subcat'])\n",
    "        subcat_arr = le.fit_transform(self.train_df['subcat'])\n",
    "        subcat_df = pd.DataFrame(data=subcat_arr,columns=le.classes_)    \n",
    "        self.Y = pd.concat([cat_df,subcat_df],axis=1)        \n",
    "        self.Y.drop('NaN',inplace=True,axis=1)\n",
    "        #print('Y value', self.Y.columns)\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \n",
    "        col_todrop = ['cat','subcat','Unnamed: 0','document_id']\n",
    "        for col in col_todrop:\n",
    "            self.train_df.drop(col,axis=1,inplace=True)          \n",
    "            \n",
    "    def prepare_validationset(self):\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(self.train_df['content'], self.Y, \n",
    "                                                            test_size=0.33, random_state=42)                 \n",
    "        return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "    def createmodel(self):\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test =  self.prepare_validationset()\n",
    "        #X_train, X_test, Y_train, Y_test = train_test_split(self.train_df['content'], self.Y,test_size=0.33, random_state=42)                             \n",
    "        X_train_transposed =X_train.T\n",
    "        self.pipe = Pipeline([\n",
    "                    ('cv', CountVectorizer(ngram_range=(1, 3))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('LP_GNB', LabelPowerset(GaussianNB()))])\n",
    "        # train\n",
    "        self.pipe.fit(X_train_transposed, Y_train)\n",
    "        \n",
    "        # predict\n",
    "        prediction = self.pipe.predict(X_test)\n",
    "         \n",
    "        print('validation dataset accuracy is {}'.format(accuracy_score(Y_test, prediction)))           \n",
    "        print('Hamming loss is {}'.format(hamming_loss(Y_test, prediction)))\n",
    "        print('F1 Score is {}'.format(f1_score(Y_test, prediction,average='samples')))\n",
    "        \n",
    "        return()\n",
    "    \n",
    "    def termfreq(self,n_comp):\n",
    "        \n",
    "        tfidfvec = TfidfVectorizer(ngram_range=(1,3), strip_accents='unicode',\n",
    "                       lowercase =True, analyzer='word', token_pattern=r'\\w+',\n",
    "                       stop_words = 'english')\n",
    "        \n",
    "        tfidf = tfidfvec.fit_transform(self.train_df['content'])  \n",
    "        self.train_df.drop('content',inplace=True,axis=1)\n",
    "        self.train_df = pd.DataFrame(tfidf.toarray(), columns=tfidfvec.get_feature_names())\n",
    "        \n",
    "        '''\n",
    "        df1 = pd.DataFrame(tfidf.toarray(), columns=tfidfvec.get_feature_names())\n",
    "        self.train_df = pd.concat([self.train_df, df1], axis=1)\n",
    "        svdT = TruncatedSVD(n_components=n_comp)\n",
    "        self.train_df = svdT.fit_transform(self.train_df) \n",
    "        \n",
    "        \n",
    "        test_tfidf = tfidfvec.transform(self.test_df['content'])\n",
    "        self.test_df.drop('content',inplace=True,axis=1)\n",
    "        df1 = pd.DataFrame(test_tfidf.toarray(), columns=tfidfvec.get_feature_names())\n",
    "        self.test_df = pd.concat([self.test_df, df1], axis=1)\n",
    "        self.test_df = svdT.transform(self.test_tfidf)                 \n",
    "        '''\n",
    "        \n",
    "    def createmodel_2(self):\n",
    "                        \n",
    "        self.termfreq()                      \n",
    "        clf = LabelPowerset(GaussianNB())\n",
    "        X_train, X_test, Y_train, Y_test =  self.prepare_validationset()\n",
    "        clf.fit(X_train,Y_train)\n",
    "        prediction = clf.predict(X_test)\n",
    "        \n",
    "        # predict\n",
    "        # prediction = self.pipe.predict(X_test)\n",
    "         \n",
    "        print('validation dataset accuracy is {}'.format(accuracy_score(Y_test, prediction)))           \n",
    "        print('Hamming loss is {}'.format(hamming_loss(Y_test, prediction)))\n",
    "        print('F1 Score is {}'.format(f1_score(Y_test, prediction,average='samples')))\n",
    "        \n",
    "        return()\n",
    "    \n",
    "    def prepare_testdata(self,fname):\n",
    "        \n",
    "        self.test_df = pd.read_csv(fname)         \n",
    "        self.test_id = self.test_df['document_id']\n",
    "        \n",
    "        col_todrop = ['document_id','Unnamed: 0']\n",
    "        for col in col_todrop:\n",
    "            self.test_df.drop(col,axis=1,inplace=True)            \n",
    "        \n",
    "    \n",
    "    def predict_testdataset(self):\n",
    "         \n",
    "        test_predicitions = self.pipe.predict(self.test_df['content'])         \n",
    "        return(test_predicitions)\n",
    "    \n",
    "    \n",
    "    def get_testdata(self):\n",
    "        return(self.test_df)\n",
    "    \n",
    "    def get_traindata(self):\n",
    "        return(self.train_df)\n",
    "    \n",
    "    def get_target(self):\n",
    "        return(self.Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = trainmodel()\n",
    "model.transform_target()\n",
    " \n",
    "model.prepare_data()\n",
    "\n",
    "\n",
    "#Read the data for which the label to be predicted......\n",
    "model.prepare_testdata(\"../../data/processed/test.csv\")\n",
    "\n",
    "#test_result is the predicted label \n",
    "test_result = model.predict_testdataset()\n",
    "\n",
    "#Convert the binary n-array to the corresponding label\n",
    "#resultant array will have two columns - cat, subcat\n",
    "\n",
    "data = np.zeros(shape=[test_result.shape[0],2])\n",
    "data_str = data.astype(str)\n",
    "\n",
    "cx = scipy.sparse.coo_matrix(test_result)\n",
    "for i,j,v in zip(cx.row, cx.col, cx.data):      \n",
    "    if(data_str[i][0] == '0.0'):\n",
    "        #print(\"Inside if stmt ...... \",i,j)\n",
    "        data_str[i][0] = commonfunction.getlabelname(j)\n",
    "    else:\n",
    "        #print(\"Inside if else ...... \",i,j)\n",
    "        data_str[i][1] = commonfunction.getlabelname(j)         \n",
    "\n",
    "test_df = pd.read_csv(\"../../data/processed/test.csv\")\n",
    "test_id = test_df['document_id']\n",
    "label_df = pd.DataFrame({' ':test_id,'cat':data_str[:,0],'subcat':data_str[:,1]})\n",
    "label_df.to_csv(\"../../data/processed/predicted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X): \n",
    "        return X[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = trainmodel()\n",
    "model.transform_target()\n",
    "model.prepare_data()\n",
    "\n",
    "train_df  = model.get_traindata()\n",
    "target = model.get_target()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_df,target, test_size=0.33, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word',min_df=0.3,\n",
    "                              norm='l2',ngram_range=(1,3),sublinear_tf=True, use_idf=True)\n",
    "tfidf_vec = tfidf.fit_transform(train_df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss is 0.0363957030624\n",
      "validation dataset accuracy is 0.75468975469\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipeline1 = Pipeline([\n",
    "    ('select_column', ColumnSelector(key='content')),\n",
    "    ('tfidf', TfidfVectorizer(analyzer='word',\n",
    "                              norm='l2',ngram_range=(1,3),sublinear_tf=True, use_idf=True)),\n",
    "    ( 'svd', TruncatedSVD() ),\n",
    "    ( 'LP_GNB', LabelPowerset(GaussianNB()) )\n",
    "    \n",
    "])\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('select_column', ColumnSelector(key='content')),\n",
    "    ('cvec',CountVectorizer(ngram_range=(1,3))),\n",
    "    ( 'svd', TruncatedSVD() ),\n",
    "    ('classifier',LabelPowerset(XGBClassifier())),        \n",
    "])\n",
    " \n",
    "# this is where you define the values for\n",
    "# GridSearchCV to iterate over\n",
    "param_grid = {      \n",
    "    \n",
    "    'svd__n_components': [500,1000],\n",
    "    #so called `eta` value  \n",
    "    'classifier__classifier__max_depth': [6],\n",
    "    'classifier__classifier__learning_rate': [0.05,0.001],\n",
    "    'classifier__classifier__max_depth': [6],\n",
    "    'classifier__classifier__min_child_weight': [11],              \n",
    "    'classifier__classifier__subsample': [0.8],\n",
    "    'classifier__classifier__colsample_bytree': [0.7],\n",
    "    'classifier__classifier__n_estimators': [100]   \n",
    "}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline2, cv=3, param_grid=param_grid) \n",
    "grid.fit(X_train,Y_train)\n",
    "# test the classifier\n",
    "prediction = grid.predict(X_test)\n",
    "print('Hamming loss is {}'.format(hamming_loss(Y_test, prediction)))\n",
    "print('validation dataset accuracy is {}'.format(accuracy_score(Y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaler', PipelineHelper([\n",
    "        ('std', StandardScaler()),\n",
    "        ('max', MaxAbsScaler()),\n",
    "    ])),\n",
    "    ('classifier', PipelineHelper([\n",
    "        ('svm', LinearSVC()),\n",
    "        ('rf', RandomForestClassifier()),\n",
    "    ])),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'scaler__selected_model': pipe.named_steps['scaler'].generate({\n",
    "        'std__with_mean': [True, False],\n",
    "        'std__with_std': [True, False],\n",
    "        'max__copy': [True],  # just for displaying\n",
    "    }),\n",
    "    'classifier__selected_model': pipe.named_steps['classifier'].generate({\n",
    "        'svm__C': [0.1, 1.0],\n",
    "        'rf__n_estimators': [100, 20],\n",
    "    })\n",
    "}\n",
    "grid = GridSearchCV(pipe, params, scoring='accuracy', verbose=1)\n",
    "grid.fit(X_iris, y_iris)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.get_params of Pipeline(memory=None,\n",
       "     steps=[('select_column', ColumnSelector(key='content')), ('cvec', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, ...ol=0.0)), ('LP_GNB', LabelPowerset(classifier=GaussianNB(priors=None), require_dense=[True, True]))])>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start training and prediction.......\n",
    "model = trainmodel()\n",
    "model.transform_target()\n",
    "model.prepare_data()\n",
    "print(\"------ in tf-idf --------\")\n",
    "model.termfreq(200)\n",
    "\n",
    "train_df = model.get_traindata()\n",
    "target = model.get_target()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_df,target, test_size=0.33, random_state=42) \n",
    "\n",
    "# train the classifier\n",
    "model = pipe.fit(X_train.T, Y_train)\n",
    "\n",
    "# test the classifier\n",
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss is 0.0599647266314\n",
      "validation dataset accuracy is 0.621933621934\n"
     ]
    }
   ],
   "source": [
    "# test the classifier\n",
    "prediction = grid.predict(X_test)\n",
    "print('Hamming loss is {}'.format(hamming_loss(Y_test, prediction)))\n",
    "print('validation dataset accuracy is {}'.format(accuracy_score(Y_test, prediction)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
