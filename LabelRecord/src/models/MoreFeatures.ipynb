{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayap/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string,gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import scipy, os\n",
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer,OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split \n",
    "\n",
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score,hamming_loss,f1_score,confusion_matrix\n",
    "\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "import commonfunction\n",
    "import logging\n",
    "import logging.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logging.config.fileConfig(os.path.abspath('../config/logging.conf'))\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class trainmodel:\n",
    "                    \n",
    "    def __init__(self):        \n",
    "        self.train_df = self.test_df = self.Y = pd.DataFrame()\n",
    "        self.train_df = pd.read_csv(\"../../data/processed/train.csv\")\n",
    "        self.test_df = pd.read_csv(\"../../data/processed/test.csv\")\n",
    "        logger.debug(\"The column in train set %s\", self.train_df.columns)\n",
    "        \n",
    "    def transform_target(self):\n",
    "        \n",
    "        le  = LabelBinarizer()\n",
    "        cat_arr = le.fit_transform(self.train_df['cat'])\n",
    "        cat_df = pd.DataFrame(data=cat_arr,columns=le.classes_)\n",
    "        logger.debug('cat classes %s',le.classes_)\n",
    "        \n",
    "        self.train_df['subcat'] = np.where(pd.isnull(self.train_df['subcat']),'NaN',self.train_df['subcat'])\n",
    "        subcat_arr = le.fit_transform(self.train_df['subcat'])\n",
    "        subcat_df = pd.DataFrame(data=subcat_arr,columns=le.classes_)\n",
    "        logger.debug('subcat classes %s',le.classes_)\n",
    "        self.Y = pd.concat([cat_df,subcat_df],axis=1)        \n",
    "        #self.Y.drop('NaN',inplace=True,axis=1)\n",
    "        logger.debug('Y value %s', self.Y.columns)\n",
    "        \n",
    "    def removeCol_fromtrain(self):\n",
    "        \n",
    "        col_todrop = ['cat','subcat','Unnamed: 0','document_id','no_pages', u'ques_cnt', u'no_words']\n",
    "        for col in col_todrop:\n",
    "            self.train_df.drop(col,axis=1,inplace=True)          \n",
    "            \n",
    "    def prepare_validationset(self):\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(self.train_df['content'], self.Y, \n",
    "                                                            test_size=0.33, random_state=42)                 \n",
    "        return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "    def createmodel(self):\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test =  self.prepare_validationset()         \n",
    "        X_train_transposed =X_train.T\n",
    "        self.pipe = Pipeline([\n",
    "                    ('cv', CountVectorizer(ngram_range=(1, 3))),\n",
    "                    ('tfidf', TfidfTransformer( use_idf=False)),\n",
    "                    ('LP_GNB', LabelPowerset(GaussianNB()))])\n",
    "        # train\n",
    "        self.pipe.fit(X_train_transposed, Y_train)\n",
    "        \n",
    "        # predict\n",
    "        prediction = self.pipe.predict(X_test)\n",
    "         \n",
    "        print('validation dataset accuracy is {}'.format(accuracy_score(Y_test, prediction)))           \n",
    "        print('Hamming loss is {}'.format(hamming_loss(Y_test, prediction)))\n",
    "        print('F1 Score is {}'.format(f1_score(Y_test, prediction,average='samples')))\n",
    "        print('---- confusion matrix ----- ')\n",
    "        \n",
    "        #valset_pred = prediction.todense()\n",
    "        \n",
    "        self.print_confusionmatrix(Y_test,prediction)                 \n",
    "        \n",
    "        return()\n",
    "    \n",
    "    def termfreq(self):\n",
    "        \n",
    "        tfidfvec = TfidfVectorizer(ngram_range=(1,3), strip_accents='unicode',\n",
    "                       lowercase =True, analyzer='word', token_pattern=r'\\w+',\n",
    "                       stop_words = 'english')\n",
    "        \n",
    "        full_tfidf = tfidfvec.fit_transform(self.train_df['content'].values.tolist() + self.test_df['content'].values.tolist())\n",
    "        train_tfidf = tfidfvec.fit_transform(self.train_df['content'])  \n",
    "        self.train_df.drop('content',inplace=True,axis=1)\n",
    "        logger.debug(\"---- The size of train_tfidf vector is ------ %s \",train_tfidf.shape)\n",
    "        \n",
    "        test_tfidf = tfidfvec.fit_transform(self.test_df['content'])  \n",
    "        self.test_df.drop('content',inplace=True,axis=1)                 \n",
    "         \n",
    "        n_comp = 750\n",
    "        svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "        svd_obj.fit(full_tfidf)\n",
    "        train_svd = pd.DataFrame(svd_obj.fit_transform(train_tfidf))\n",
    "        test_svd = pd.DataFrame(svd_obj.fit_transform(test_tfidf))\n",
    "    \n",
    "        train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "        test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "        #print('Train dataset head', self.train_df.head(10))\n",
    "        self.train_df = pd.concat([self.train_df, train_svd], axis=1)\n",
    "        self.test_df = pd.concat([self.test_df, test_svd], axis=1)\n",
    "        #print('Train dataset head', self.train_df.head(10))\n",
    "        del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd\n",
    "        gc.collect()\n",
    "         \n",
    "    def createmodel_2(self):\n",
    "                        \n",
    "        self.termfreq()              \n",
    "         \n",
    "        self.clf = LabelPowerset(GaussianNB())\n",
    "        X_train, X_test, Y_train, Y_test =  self.prepare_validationset()\n",
    "        self.clf.fit(X_train,Y_train)\n",
    "        prediction = self.clf.predict(X_test)\n",
    "        \n",
    "        # predict\n",
    "        # prediction = self.pipe.predict(X_test)\n",
    "         \n",
    "        print('validation dataset accuracy is {}'.format(accuracy_score(Y_test, prediction)))           \n",
    "        print('Hamming loss is {}'.format(hamming_loss(Y_test, prediction)))\n",
    "        print('F1 Score is {}'.format(f1_score(Y_test, prediction,average='samples')))\n",
    "       \n",
    "        return()\n",
    "    \n",
    "    def prepare_testdata(self,fname):\n",
    "        \n",
    "        \n",
    "        logger.debug(\"--- Before ---- %s\",self.test_df.shape)\n",
    "        self.test_id = self.test_df['document_id']\n",
    "        \n",
    "        col_todrop = ['document_id','Unnamed: 0']\n",
    "        for col in col_todrop:\n",
    "            self.test_df.drop(col,axis=1,inplace=True)\n",
    "            \n",
    "        logger.debug(\"--- After ---- %s\",self.test_df.shape)\n",
    "        \n",
    "    \n",
    "    def predict_testdataset(self):\n",
    "         \n",
    "        #test_predicitions = self.pipe.predict(self.test_df['content'])\n",
    "        print(\"The testdataset \", self.test_df.columns)\n",
    "        test_predicitions = self.clf.predict(self.test_df)\n",
    "        logger.debug('test predictions shape %s', test_predicitions.shape)\n",
    "        \n",
    "        return(test_predicitions)\n",
    "    \n",
    "    def predict_testdataset_1(self):\n",
    "         \n",
    "        #test_predicitions = self.pipe.predict(self.test_df['content'])\n",
    "        print(\"The testdataset \", self.test_df.columns,\"the testdataset shape \", self.test_df.shape)\n",
    "        test_predicitions = self.pipe.predict(self.test_df['content'])\n",
    "        print(\"Test Prediction size ==== \", test_predicitions.shape)\n",
    "        logger.debug('test predictions shape %s', test_predicitions.shape)       \n",
    "        \n",
    "        return(test_predicitions)\n",
    "    \n",
    "    \n",
    "    def get_testdata(self):\n",
    "        return(self.test_df)\n",
    "    \n",
    "    def get_traindata(self):\n",
    "        return(self.train_df)\n",
    "    \n",
    "    def get_binarizedactuals(self):\n",
    "        \n",
    "        act_lab = pd.read_csv(\"../../data/processed/test_label.csv\")\n",
    "        act_labels = act_lab[['cat','subcat']]\n",
    "\n",
    "        le  = LabelBinarizer()\n",
    "        cat_arr = le.fit_transform(act_labels['cat'])\n",
    "        cat_df = pd.DataFrame(data=cat_arr,columns=le.classes_)        \n",
    "        act_labels['subcat'] = np.where(pd.isnull(act_labels['subcat']),'NaN',act_labels['subcat'])\n",
    "        subcat_arr = le.fit_transform(act_labels['subcat'])\n",
    "        subcat_df = pd.DataFrame(data=subcat_arr,columns=le.classes_)\n",
    "        actual_y = pd.concat([cat_df,subcat_df],axis=1) \n",
    "        \n",
    "        return(actual_y)\n",
    "    \n",
    "    def print_confusionmatrix(self,actual,predicted):\n",
    "        \n",
    "        predicted = predicted.todense()\n",
    "        for i in range(actual.shape[1]):\n",
    "            print(\"Confusion matrix of {}\".format(actual.columns[i]))\n",
    "            print(confusion_matrix(actual.iloc[:,i], predicted[:,i]))             \n",
    "            print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-23 15:13:46,146 - __main__ - INFO - Training starts ...... \n",
      "2018-07-23 15:13:46,181 - __main__ - DEBUG - The column in train set Index([u'Unnamed: 0', u'content', u'document_id', u'no_pages', u'ques_cnt',\n",
      "       u'no_words', u'cat', u'subcat'],\n",
      "      dtype='object')\n",
      "2018-07-23 15:13:46,185 - __main__ - DEBUG - cat classes ['Billing' 'Clinic/Doctors Office' 'Employment' 'Hospitalization'\n",
      " 'InsuranceDisability' 'Military' 'NRLs']\n",
      "2018-07-23 15:13:46,190 - __main__ - DEBUG - subcat classes ['CardiacData' 'Consents' 'Correspondence' 'EmergencyRoomRecord'\n",
      " 'Insurance/DisabilityCorrespondence' 'LaboratoryReports' 'NaN'\n",
      " 'Nurses/Notes/TelephoneLogs' 'Physicians/Orders'\n",
      " 'Prescriptions/MedicationLists' 'ProgressNotes'\n",
      " 'X-Rays/MRIs/CTscansRadiologyQuestionnaries']\n",
      "2018-07-23 15:13:46,191 - __main__ - DEBUG - Y value Index([u'Billing', u'Clinic/Doctors Office', u'Employment', u'Hospitalization',\n",
      "       u'InsuranceDisability', u'Military', u'NRLs', u'CardiacData',\n",
      "       u'Consents', u'Correspondence', u'EmergencyRoomRecord',\n",
      "       u'Insurance/DisabilityCorrespondence', u'LaboratoryReports', u'NaN',\n",
      "       u'Nurses/Notes/TelephoneLogs', u'Physicians/Orders',\n",
      "       u'Prescriptions/MedicationLists', u'ProgressNotes',\n",
      "       u'X-Rays/MRIs/CTscansRadiologyQuestionnaries'],\n",
      "      dtype='object')\n",
      "validation dataset accuracy is 0.819624819625\n",
      "Hamming loss is 0.0274929748614\n",
      "F1 Score is 0.869408369408\n",
      "---- confusion matrix ----- \n",
      "Confusion matrix of Billing\n",
      "[[650   6]\n",
      " [  5  32]]\n",
      "\n",
      "Confusion matrix of Clinic/Doctors Office\n",
      "[[492  24]\n",
      " [ 36 141]]\n",
      "\n",
      "Confusion matrix of Employment\n",
      "[[650   7]\n",
      " [  7  29]]\n",
      "\n",
      "Confusion matrix of Hospitalization\n",
      "[[348  34]\n",
      " [ 21 290]]\n",
      "\n",
      "Confusion matrix of InsuranceDisability\n",
      "[[638   7]\n",
      " [ 10  38]]\n",
      "\n",
      "Confusion matrix of Military\n",
      "[[650   5]\n",
      " [  2  36]]\n",
      "\n",
      "Confusion matrix of NRLs\n",
      "[[646   1]\n",
      " [  3  43]]\n",
      "\n",
      "Confusion matrix of CardiacData\n",
      "[[677   0]\n",
      " [  4  12]]\n",
      "\n",
      "Confusion matrix of Consents\n",
      "[[607  17]\n",
      " [  6  63]]\n",
      "\n",
      "Confusion matrix of Correspondence\n",
      "[[653   1]\n",
      " [  2  37]]\n",
      "\n",
      "Confusion matrix of EmergencyRoomRecord\n",
      "[[624  28]\n",
      " [  4  37]]\n",
      "\n",
      "Confusion matrix of Insurance/DisabilityCorrespondence\n",
      "[[638   7]\n",
      " [ 10  38]]\n",
      "\n",
      "Confusion matrix of LaboratoryReports\n",
      "[[620   2]\n",
      " [  6  65]]\n",
      "\n",
      "Confusion matrix of NaN\n",
      "[[520  16]\n",
      " [ 14 143]]\n",
      "\n",
      "Confusion matrix of Nurses/Notes/TelephoneLogs\n",
      "[[598  15]\n",
      " [ 19  61]]\n",
      "\n",
      "Confusion matrix of Physicians/Orders\n",
      "[[650   2]\n",
      " [  5  36]]\n",
      "\n",
      "Confusion matrix of Prescriptions/MedicationLists\n",
      "[[662   5]\n",
      " [ 10  16]]\n",
      "\n",
      "Confusion matrix of ProgressNotes\n",
      "[[676   0]\n",
      " [  5  12]]\n",
      "\n",
      "Confusion matrix of X-Rays/MRIs/CTscansRadiologyQuestionnaries\n",
      "[[601   4]\n",
      " [ 12  76]]\n",
      "\n",
      "('The testdataset ', Index([u'Unnamed: 0', u'content', u'document_id', u'no_pages', u'ques_cnt',\n",
      "       u'no_words'],\n",
      "      dtype='object'), 'the testdataset shape ', (1035, 6))\n",
      "('Test Prediction size ==== ', (1035, 19))\n",
      "2018-07-23 15:14:28,257 - __main__ - DEBUG - test predictions shape (1035, 19)\n",
      "Confusion matrix of Billing\n",
      "[[964  10]\n",
      " [ 15  46]]\n",
      "\n",
      "Confusion matrix of Clinic/Doctors Office\n",
      "[[733  33]\n",
      " [ 53 216]]\n",
      "\n",
      "Confusion matrix of Employment\n",
      "[[970   4]\n",
      " [ 17  44]]\n",
      "\n",
      "Confusion matrix of Hospitalization\n",
      "[[500  64]\n",
      " [ 29 442]]\n",
      "\n",
      "Confusion matrix of InsuranceDisability\n",
      "[[957  14]\n",
      " [ 11  53]]\n",
      "\n",
      "Confusion matrix of Military\n",
      "[[976   7]\n",
      " [  4  48]]\n",
      "\n",
      "Confusion matrix of NRLs\n",
      "[[976   2]\n",
      " [  5  52]]\n",
      "\n",
      "Confusion matrix of CardiacData\n",
      "[[1000    1]\n",
      " [   3   31]]\n",
      "\n",
      "Confusion matrix of Consents\n",
      "[[884  32]\n",
      " [  6 113]]\n",
      "\n",
      "Confusion matrix of Correspondence\n",
      "[[970   2]\n",
      " [  7  56]]\n",
      "\n",
      "Confusion matrix of EmergencyRoomRecord\n",
      "[[943  29]\n",
      " [ 11  52]]\n",
      "\n",
      "Confusion matrix of Insurance/DisabilityCorrespondence\n",
      "[[957  14]\n",
      " [ 11  53]]\n",
      "\n",
      "Confusion matrix of LaboratoryReports\n",
      "[[906   4]\n",
      " [  7 118]]\n",
      "\n",
      "Confusion matrix of NaN\n",
      "[[785  19]\n",
      " [ 37 194]]\n",
      "\n",
      "Confusion matrix of Nurses/Notes/TelephoneLogs\n",
      "[[904  32]\n",
      " [ 20  79]]\n",
      "\n",
      "Confusion matrix of Physicians/Orders\n",
      "[[984   1]\n",
      " [  7  43]]\n",
      "\n",
      "Confusion matrix of Prescriptions/MedicationLists\n",
      "[[992   2]\n",
      " [ 14  27]]\n",
      "\n",
      "Confusion matrix of ProgressNotes\n",
      "[[1016    0]\n",
      " [   6   13]]\n",
      "\n",
      "Confusion matrix of X-Rays/MRIs/CTscansRadiologyQuestionnaries\n",
      "[[903   5]\n",
      " [ 12 115]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayap/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Start training and prediction.......\n",
    "\n",
    "logger.info(\"Training starts ...... \")\n",
    "\n",
    "model = trainmodel()\n",
    "model.transform_target()\n",
    "model.removeCol_fromtrain()\n",
    "model.createmodel() \n",
    "\n",
    "#test_result is the predicted label \n",
    "test_result = model.predict_testdataset_1()\n",
    "\n",
    "#Print the confusion matrix for the test dataset. \n",
    "res_y = model.get_binarizedactuals()\n",
    "model.print_confusionmatrix(res_y,test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert the binary n-array to the corresponding label -\n",
    "#dataset will be of shape - [len,col], col -(cat,subcat)\n",
    " \n",
    "#The sparse predicted array is converted to array of size(len,col)    \n",
    "data = np.zeros(shape=[test_result.shape[0],2])\n",
    "data_str = data.astype(str)\n",
    "\n",
    "cx = scipy.sparse.coo_matrix(test_result)\n",
    "for i,j,v in zip(cx.row, cx.col, cx.data):      \n",
    "    if(data_str[i][0] == '0.0'):\n",
    "        #print(\"Inside if stmt ...... \",i,j)\n",
    "        data_str[i][0] = commonfunction.getlabelname(j)\n",
    "    else:\n",
    "        #print(\"Inside if else ...... \",i,j)\n",
    "        data_str[i][1] = commonfunction.getlabelname(j)\n",
    "     \n",
    "    \n",
    "\n",
    "# Write the prediction into the file - ../../data/processed/predicted.csv\n",
    "test_df = pd.read_csv(\"../../data/processed/test.csv\")\n",
    "test_id = test_df['document_id']\n",
    "label_df = pd.DataFrame({' ':test_id,'cat':data_str[:,0],'subcat':data_str[:,1]})\n",
    "label_df.to_csv(\"../../data/processed/predicted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "f,ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, linewidths=0.5,linecolor=\"red\", fmt= '.1f',ax=ax)\n",
    "plt.show()\n",
    "plt.savefig('graph.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
